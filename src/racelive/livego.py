""" ---------------------------------------------------------------------------------------------------------
    Super Formula Race Live   Ver : 1.2.4  2025.06

    python src/racelive/livego.py

    ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ã‚’è¨˜è¿°ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚
    åˆ¶å¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›£è¦–ã—ã¦ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    ã“ã“ã§ã¯ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹å‡¦ç†ã‚’è¨˜è¿°ã—ã¦ã„ã¾ã™ã€‚

    1. åˆ¶å¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›£è¦–ã—ã¦ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    2. SuperFormulaã®RaceLiveã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ã€‚
    3. racelive.jpã®ã‚µãƒ¼ãƒ“ã‚¹ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ãªã®ã§åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®SuperFormula Lightsã‚‚èª­ã¿è¾¼ã¿å¯èƒ½ã€‚
    4. ã‚¦ã‚§ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã‚‚ã€ã“ã“ã«è¨˜è¿°ã€‚

----------------------------------------------------------------------------------------------------------"""
import os
import sys
from pathlib import Path
import time
import json
import threading
import schedule
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import traceback

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’sys.pathã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
src_path = project_root / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

from racelive.scraperead import Racelivescraper
from racelive.utils import Datalist

# åˆ¶å¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
control_file = "./data/scraping_control.json"

def check_scraping_control():
    """åˆ¶å¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çŠ¶æ…‹ã‚’å–å¾—"""
    if os.path.exists(control_file):
        try:
            with open(control_file, "r", encoding="utf-8") as f:
                control_data = json.load(f)
            return control_data.get("scraping", False)
        except:
            return False
    return False

def update_scraping_status(status, message=""):
    """åˆ¶å¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çŠ¶æ…‹ã‚’æ›´æ–°"""
    try:
        if os.path.exists(control_file):
            with open(control_file, "r", encoding="utf-8") as f:
                control_data = json.load(f)
        else:
            control_data = {}
        
        control_data["scraping"] = status
        control_data["timestamp"] = datetime.now().isoformat()
        control_data["status_from"] = "livego.py"
        if message:
            control_data["message"] = message
        
        os.makedirs(os.path.dirname(control_file), exist_ok=True)
        with open(control_file, "w", encoding="utf-8") as f:
            json.dump(control_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"åˆ¶å¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

def load_settings():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    try:
        with open("./data/racelive.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def main():
    print("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç›£è¦–ã‚’é–‹å§‹ã—ã¾ã—ãŸ...")
    print("Ctrl+C ã§çµ‚äº†")
    
    scraping_active = False
    scheduler = schedule.Scheduler()
    scheduled_job = None
    
    while True:
        try:
            # åˆ¶å¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
            should_scraping = check_scraping_control()
            
            if should_scraping and not scraping_active:
                # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹
                print("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹æŒ‡ç¤ºã‚’å—ä¿¡")
                
                # è¨­å®šèª­ã¿è¾¼ã¿
                sf_setfile = load_settings()
                if not sf_setfile:
                    print("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“")
                    time.sleep(5)
                    continue
                
                # è¨­å®šå€¤å–å¾—
                category = sf_setfile["Category"][sf_setfile["Last Category"]]["Name"]
                category_index = sf_setfile["Last Category"]
                url = sf_setfile["Category"][sf_setfile["Last Category"]]["URL"]
                circuit = sf_setfile["Circuit"][sf_setfile["Last Circuit"]]["Name"]
                race_lap = sf_setfile["Race Lap"]
                sector = sf_setfile["Circuit"][sf_setfile["Last Circuit"]]["Sector"]
                
                time_path = sf_setfile["Time Path"]
                lastevent = sf_setfile["Last Event"]
                
                session_name = sf_setfile["Session"][sf_setfile["Last Session"]]["Name"]
                session_starttime = sf_setfile["Last StartTime"]
                session_endtime = sf_setfile["Last EndTime"]
                session_date = datetime.now().strftime("%Y/%m/%d")
                
                # ã‚ªãƒ•ã‚»ãƒƒãƒˆå€¤ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®šï¼‰
                offset_start = sf_setfile.get("Offset Start", 1)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1åˆ†
                offset_end = sf_setfile.get("Offset End", 3)      # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3åˆ†
                
                # ãƒ‡ãƒ¼ã‚¿æº–å‚™
                datal = Datalist(sf_setfile["Last Category"])
                teamlist, mk, mk2 = datal.teamlist()
                driver_list, car_no_list = datal.driverlist()
                df0 = datal.data_db(race_lap)
                
                save_path = os.path.join(time_path, f"{lastevent}_{session_name}")
                
                print(f"ã‚«ãƒ†ã‚´ãƒª: {category}")
                print(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³: {session_name}")
                print(f"æ™‚é–“: {session_starttime} - {session_endtime}")
                print(f"ã‚ªãƒ•ã‚»ãƒƒãƒˆ: é–‹å§‹-{offset_start}åˆ†, çµ‚äº†+{offset_end}åˆ†")
                print(f"ä¿å­˜å…ˆ: {save_path}")
                
                # æ™‚é–“è¨ˆç®—
                try:
                    session_start_str = f"{session_date} {session_starttime}:00"
                    session_start_dt = datetime.strptime(session_start_str, "%Y/%m/%d %H:%M:%S")
                    
                    session_end_str = f"{session_date} {session_endtime}:00"
                    session_end_dt = datetime.strptime(session_end_str, "%Y/%m/%d %H:%M:%S")
                    
                    # è¨­å®šã•ã‚ŒãŸã‚ªãƒ•ã‚»ãƒƒãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æœŸé–“ã‚’è¨ˆç®—
                    scraping_start_dt = session_start_dt - timedelta(minutes=offset_start)
                    scraping_end_dt = session_end_dt + timedelta(minutes=offset_end)
                    
                    # scraper.livetimeã«ã¯ã‚ªãƒ•ã‚»ãƒƒãƒˆå‰ã®å…ƒã®æ™‚é–“ã‚’ä½¿ç”¨
                    original_session_start_timestamp = int(time.mktime(session_start_dt.timetuple()))
                    original_session_end_timestamp = int(time.mktime(session_end_dt.timetuple()))
                    
                    # çµ‚äº†æ™‚é–“ã¯ã‚ªãƒ•ã‚»ãƒƒãƒˆå¾Œã®æ™‚é–“ã‚’ä½¿ç”¨
                    scraping_end_timestamp = int(time.mktime(scraping_end_dt.timetuple()))
                    
                    now = datetime.now()
                    
                    if now > scraping_end_dt:
                        print("âš ï¸ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ‚äº†æ™‚åˆ»ã‚’éãã¦ã„ã¾ã™")
                        update_scraping_status(False, "çµ‚äº†æ™‚åˆ»ã‚’éãã¦ã„ã¾ã™")
                        time.sleep(5)
                        continue
                    
                    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œé–¢æ•°
                    def run_scraping():
                        try:
                            print("ğŸ“¡ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
                            print(f"ğŸ“… ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“: {session_start_dt} - {session_end_dt}")
                            print(f"ğŸ•’ å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æœŸé–“: {scraping_start_dt} - {scraping_end_dt}")
                            update_scraping_status(True, "å®Ÿè¡Œä¸­")
                            
                            # timedataã¨ã—ã¦å…ƒã®session_start_dtã®æƒ…å ±ã‚’ä¿æŒ
                            timedata = session_start_dt
                            
                            scraper = Racelivescraper(url, df0, category_index, sector, car_no_list, driver_list, mk, save_path)
                            # é–‹å§‹æ™‚é–“ï¼šã‚ªãƒ•ã‚»ãƒƒãƒˆå‰ã€çµ‚äº†æ™‚é–“ï¼šã‚ªãƒ•ã‚»ãƒƒãƒˆå¾Œ
                            scraper.livetime(original_session_start_timestamp, scraping_end_timestamp)
                            print("âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†")
                            update_scraping_status(False, "å®Œäº†")
                        except Exception as e:
                            print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                            traceback.print_exc()
                            update_scraping_status(False, f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    
                    # ã‚ªãƒ•ã‚»ãƒƒãƒˆå¾Œã®scraping_start_dtã®æ™‚é–“ã«ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
                    schedule_time = scraping_start_dt.strftime("%H:%M")
                    scheduled_job = scheduler.every().day.at(schedule_time).do(run_scraping)
                    
                    if now < scraping_start_dt:
                        remaining_time = scraping_start_dt - now
                        print(f"â° ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ã¾ã§: {remaining_time}")
                        print(f"ğŸ“… ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š: {schedule_time}ã«å®Ÿè¡Œï¼ˆã‚ªãƒ•ã‚»ãƒƒãƒˆå¾Œã®æ™‚åˆ»ï¼‰")
                        update_scraping_status(True, f"é–‹å§‹äºˆå®š: {schedule_time}")
                    else:
                        print("â–¶ï¸ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ™‚é–“ä¸­ - å³åº§ã«å®Ÿè¡Œ")
                        run_scraping()
                    
                except ValueError as e:
                    print(f"æ™‚åˆ»ã®å½¢å¼ã‚¨ãƒ©ãƒ¼: {e}")
                    original_session_start_timestamp = int(time.mktime(datetime.now().timetuple()))
                    scraping_end_timestamp = int(time.mktime((datetime.now() + timedelta(minutes=2)).timetuple()))
                
                scraping_active = True
                
            elif not should_scraping and scraping_active:
                # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°åœæ­¢
                print("ğŸ›‘ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°åœæ­¢æŒ‡ç¤ºã‚’å—ä¿¡")
                if scheduled_job:
                    scheduler.cancel_job(scheduled_job)
                    scheduled_job = None
                scraping_active = False
                update_scraping_status(False, "æ‰‹å‹•åœæ­¢")
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’å®Ÿè¡Œ
            if scraping_active:
                scheduler.run_pending()
            
            time.sleep(1)  # 1ç§’é–“éš”ã§ãƒã‚§ãƒƒã‚¯
            
        except KeyboardInterrupt:
            print("\nğŸ”š çµ‚äº†ã—ã¾ã™...")
            if scraping_active:
                update_scraping_status(False, "ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†")
            break
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            time.sleep(5)

if __name__ == "__main__":
    main()
