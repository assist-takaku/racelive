""" ---------------------------------------------------------------------------------------------------------
    Super Formula Race Live   Ver : 1.2.4  2025.06

    poetry run streamlit run src/racelive/livego.py

    ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ã‚’è¨˜è¿°ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚
    streamlitã§ã¯while Trueã®ãƒ«ãƒ¼ãƒ—ãŒã§ããªã„ã®ã§ã€åˆ¥ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    ã“ã“ã§ã¯ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹å‡¦ç†ã‚’è¨˜è¿°ã—ã¦ã„ã¾ã™ã€‚

    1. streamlitã‚’ä½¿ã£ã¦ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    2. SuperFormulaã®RaceLiveã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ã€‚
    3. racelive.jpã®ã‚µãƒ¼ãƒ“ã‚¹ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ãªã®ã§åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®SuperFormula Lightsã‚‚èª­ã¿è¾¼ã¿å¯èƒ½ã€‚
    4. ã‚¦ã‚§ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã‚‚ã€ã“ã“ã«è¨˜è¿°ã€‚

----------------------------------------------------------------------------------------------------------"""
import os
import sys
from pathlib import Path
import time
import json
import schedule
import threading
import numpy as np
import pandas as pd
import concurrent.futures
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from datetime import datetime, timedelta
import traceback
import streamlit as st

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’sys.pathã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
src_path = project_root / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

from racelive.scraperead import Racelivescraper
from racelive.utils import Datalist
from racelive import const


st.set_page_config(page_title="ãƒ©ãƒƒãƒ—ã‚¿ã‚¤ãƒ ãƒ»ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°", initial_sidebar_state="collapsed" , layout="wide")

st.title("ãƒ©ãƒƒãƒ—ã‚¿ã‚¤ãƒ ãƒ»ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»ãƒšãƒ¼ã‚¸")


# æ™‚é–“è¡¨ç¤ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
datetime.now().strftime("%Y/%m/%d %H:%M:%S")
today = datetime.now().strftime("%Y/%m/%d")

sf_setfile = json.load(open("./data/racelive.json", "r", encoding="utf-8"))

category = sf_setfile["Category"][sf_setfile["Last Category"]]["Name"]
category_index = sf_setfile["Last Category"]
url = sf_setfile["Category"][sf_setfile["Last Category"]]["URL"]
circuit = sf_setfile["Circuit"][sf_setfile["Last Circuit"]]["Name"]
race_lap = sf_setfile["Race Lap"]
sector = sf_setfile["Circuit"][sf_setfile["Last Circuit"]]["Sector"]

time_path = sf_setfile["Time Path"]
lastevent = sf_setfile["Last Event"]

# ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹ãƒ»çµ‚äº†æ™‚é–“ã‚’è¨­å®š
session_name = sf_setfile["Session"][sf_setfile["Last Session"]]["Name"]
session_starttime = sf_setfile["Last StartTime"]
session_endtime = sf_setfile["Last EndTime"]
session_date = today

# ã‚«ãƒ†ã‚´ãƒªã«åŸºã¥ã„ã¦ãƒãƒ¼ãƒ ãƒªã‚¹ãƒˆã‚’å–å¾—
datal = Datalist(sf_setfile["Last Category"])
teamlist, mk, mk2 = datal.teamlist()
driver_list, car_no_list = datal.driverlist()
# ãƒ©ã‚¤ãƒ–ãƒ‡ãƒ¼ã‚¿ç”¨ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
df0 = datal.data_db(race_lap)

ambientid = sf_setfile["ambient ID"]
ambientreadKey = sf_setfile["ambient readKey"]
weapath = sf_setfile["weather Path"]
time_path = sf_setfile["Time Path"]

save_path = os.path.join(time_path, f"{lastevent}_{session_name}")


# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
.info-box {
    background-color: #f0f2f6;
    padding: 10px;
    border-radius: 5px;
    border: 2px solid #0066cc;
    text-align: center;
    margin: 5px;
}
.info-label {
    font-weight: bold;
    font-size: 14px;
    color: #0066cc;
}
.info-value {
    font-size: 25px;
    color: #333;
    margin-top: 5px;
}
</style>
""", unsafe_allow_html=True)


# è¨­å®šæƒ…å ±è¡¨ç¤º
col_1, col_2, col_3, col_4 = st.columns([1, 1, 1, 1])

with col_1:
    st.markdown(f"""
    <div class="info-box">
        <div class="info-label">ã‚«ãƒ†ã‚´ãƒª</div>
        <div class="info-value">{category}</div>
    </div>
    """, unsafe_allow_html=True)

with col_2:
    st.markdown(f"""
    <div class="info-box">
        <div class="info-label">ã‚µãƒ¼ã‚­ãƒƒãƒˆ</div>
        <div class="info-value">{circuit}</div>
    </div>
    """, unsafe_allow_html=True)

with col_3:
    st.markdown(f"""
    <div class="info-box">
        <div class="info-label">ãƒ¬ãƒ¼ã‚¹ãƒ©ãƒƒãƒ—</div>
        <div class="info-value">{race_lap}</div>
    </div>
    """, unsafe_allow_html=True)

with col_4:
    st.markdown(f"""
    <div class="info-box">
        <div class="info-label">ã‚»ã‚¯ã‚¿ãƒ¼</div>
        <div class="info-value">{sector}</div>
    </div>
    """, unsafe_allow_html=True)


col_5, col_6, col_7, col_8 = st.columns([1, 1, 1, 1])

with col_5:
    st.markdown(f"""
    <div class="info-box">
        <div class="info-label">ã‚»ãƒƒã‚·ãƒ§ãƒ³</div>
        <div class="info-value">{session_name}</div>
    </div>
    """, unsafe_allow_html=True)

with col_6:
    st.markdown(f"""
    <div class="info-box">
        <div class="info-label">é–‹å§‹æ™‚é–“</div>
        <div class="info-value">{session_starttime}</div>
    </div>
    """, unsafe_allow_html=True)

with col_7:
    st.markdown(f"""
    <div class="info-box">
        <div class="info-label">çµ‚äº†æ™‚é–“</div>
        <div class="info-value">{session_endtime}</div>
    </div>
    """, unsafe_allow_html=True)

with col_8:
    st.write("")

st.write("---")

# åˆ¶å¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
control_file = "./data/scraping_control.json"

def check_scraping_control():
    """åˆ¶å¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çŠ¶æ…‹ã‚’å–å¾—"""
    if os.path.exists(control_file):
        try:
            with open(control_file, "r", encoding="utf-8") as f:
                control_data = json.load(f)
            return control_data.get("scraping", False)
        except:
            return False
    return False

def update_scraping_status(status):
    """åˆ¶å¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çŠ¶æ…‹ã‚’æ›´æ–°"""
    try:
        if os.path.exists(control_file):
            with open(control_file, "r", encoding="utf-8") as f:
                control_data = json.load(f)
        else:
            control_data = {}
        
        control_data["scraping"] = status
        control_data["timestamp"] = datetime.now().isoformat()
        control_data["status_from"] = "livego.py"
        
        os.makedirs(os.path.dirname(control_file), exist_ok=True)
        with open(control_file, "w", encoding="utf-8") as f:
            json.dump(control_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"åˆ¶å¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

st.write("---")

# åˆ¶å¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›£è¦–ã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’åˆ¶å¾¡
scraping = check_scraping_control()

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çŠ¶æ…‹è¡¨ç¤º
if scraping:
    st.markdown("#### ğŸŸ¢ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œä¸­")
    status_color = "green"
else:
    st.markdown("#### ğŸ”´ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°åœæ­¢ä¸­")
    status_color = "red"

# çŠ¶æ…‹è¡¨ç¤ºãƒœãƒƒã‚¯ã‚¹
st.markdown(f"""
<div style="background-color: {status_color}; padding: 10px; border-radius: 5px; color: white; text-align: center; margin: 10px 0;">
    <h3>åˆ¶å¾¡çŠ¶æ…‹: {'å®Ÿè¡Œä¸­' if scraping else 'åœæ­¢ä¸­'}</h3>
</div>
""", unsafe_allow_html=True)

# æ‰‹å‹•åˆ¶å¾¡ç”¨ã®ãƒˆã‚°ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
manual_control = st.toggle("æ‰‹å‹•åˆ¶å¾¡", key="manual_toggle")
if manual_control:
    manual_scraping = st.toggle("æ‰‹å‹•ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹/åœæ­¢", value=scraping, key="manual_scraping")
    if manual_scraping != scraping:
        update_scraping_status(manual_scraping)
        st.rerun()

if scraping:
    # æ—¢å­˜ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†
    try:
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹æ™‚é–“ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        session_start_str = f"{session_date} {session_starttime}:00"
        session_start_dt = datetime.strptime(session_start_str, "%Y/%m/%d %H:%M:%S")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚é–“ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        session_end_str = f"{session_date} {session_endtime}:00"
        session_end_dt = datetime.strptime(session_end_str, "%Y/%m/%d %H:%M:%S")
        
        # é–‹å§‹æ™‚é–“ã®1åˆ†å‰ã«è¨­å®š
        scraping_start_dt = session_start_dt - timedelta(minutes=1)

        # çµ‚äº†æ™‚é–“ã®3åˆ†å¾Œã«è¨­å®š
        scraping_end_dt = session_end_dt + timedelta(minutes=3)
        
        # UNIXã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«å¤‰æ›
        session_start = int(time.mktime(scraping_start_dt.timetuple()))
        session_end = int(time.mktime(scraping_end_dt.timetuple()))
        
        # ç¾åœ¨æ™‚åˆ»ã¨ã®æ¯”è¼ƒè¡¨ç¤º
        now = datetime.now()

        # é–‹å§‹æ™‚åˆ»ã¾ã§å¾…æ©Ÿã™ã‚‹ã‹ã©ã†ã‹ã®åˆ¤å®š
        if now < scraping_start_dt:
            remaining_time = scraping_start_dt - now
            st.info(f"â° é–‹å§‹ã¾ã§: {remaining_time}")
        elif now > scraping_end_dt:
            st.error("âš ï¸ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ‚äº†æ™‚åˆ»ã‚’éãã¦ã„ã¾ã™")
            # è‡ªå‹•çš„ã«åœæ­¢
            update_scraping_status(False)
        else:
            st.success("âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œä¸­")
        
    except ValueError as e:
        st.error(f"æ™‚åˆ»ã®å½¢å¼ã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç¾åœ¨æ™‚åˆ»ã‹ã‚‰2åˆ†é–“ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        session_start = int(time.mktime(datetime.now().timetuple()))
        session_end = int(time.mktime((datetime.now() + timedelta(minutes=2)).timetuple()))

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    try:
        scraper = Racelivescraper(url, df0, category_index, sector, car_no_list, driver_list, mk, save_path)
        timedata = scraper.livetime(session_start, session_end)
    except Exception as e:
        st.error(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯è‡ªå‹•åœæ­¢
        update_scraping_status(False)

# è‡ªå‹•ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ï¼ˆ1ç§’æ¯ã«åˆ¶å¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ï¼‰
time.sleep(1)
st.rerun()
